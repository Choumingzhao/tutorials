
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "beginner/basics/quickstart_tutorial.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_beginner_basics_quickstart_tutorial.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_beginner_basics_quickstart_tutorial.py:


`Learn the Basics <intro.html>`_ ||
**Quickstart** ||
`Tensors <tensorqs_tutorial.html>`_ ||
`Datasets & DataLoaders <data_tutorial.html>`_ ||
`Transforms <transforms_tutorial.html>`_ ||
`Build Model <buildmodel_tutorial.html>`_ ||
`Autograd <autogradqs_tutorial.html>`_ ||
`Optimization <optimization_tutorial.html>`_ ||
`Save & Load Model <saveloadrun_tutorial.html>`_

Quickstart
===================
This section runs through the API for common tasks in machine learning. Refer to the links in each section to dive deeper.

Working with data
-----------------
PyTorch has two `primitives to work with data <https://pytorch.org/docs/stable/data.html>`_:
``torch.utils.data.DataLoader`` and ``torch.utils.data.Dataset``.
``Dataset`` stores the samples and their corresponding labels, and ``DataLoader`` wraps an iterable around
the ``Dataset``.

.. GENERATED FROM PYTHON SOURCE LINES 24-31

.. code-block:: default


    import torch
    from torch import nn
    from torch.utils.data import DataLoader
    from torchvision import datasets
    from torchvision.transforms import ToTensor








.. GENERATED FROM PYTHON SOURCE LINES 32-40

PyTorch offers domain-specific libraries such as `TorchText <https://pytorch.org/text/stable/index.html>`_,
`TorchVision <https://pytorch.org/vision/stable/index.html>`_, and `TorchAudio <https://pytorch.org/audio/stable/index.html>`_,
all of which include datasets. For this tutorial, we  will be using a TorchVision dataset.

The ``torchvision.datasets`` module contains ``Dataset`` objects for many real-world vision data like
CIFAR, COCO (`full list here <https://pytorch.org/vision/stable/datasets.html>`_). In this tutorial, we
use the FashionMNIST dataset. Every TorchVision ``Dataset`` includes two arguments: ``transform`` and
``target_transform`` to modify the samples and labels respectively.

.. GENERATED FROM PYTHON SOURCE LINES 40-57

.. code-block:: default


    # Download training data from open datasets.
    training_data = datasets.FashionMNIST(
        root="data",
        train=True,
        download=True,
        transform=ToTensor(),
    )

    # Download test data from open datasets.
    test_data = datasets.FashionMNIST(
        root="data",
        train=False,
        download=True,
        transform=ToTensor(),
    )





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz
    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz

      0%|          | 0/26421880 [00:00<?, ?it/s]
      0%|          | 32768/26421880 [00:00<01:26, 305544.15it/s]
      0%|          | 65536/26421880 [00:00<01:27, 300909.05it/s]
      0%|          | 131072/26421880 [00:00<01:00, 435730.25it/s]
      1%|          | 229376/26421880 [00:00<00:42, 616745.44it/s]
      2%|1         | 425984/26421880 [00:00<00:25, 1039192.50it/s]
      2%|2         | 557056/26421880 [00:00<00:23, 1089751.70it/s]
      3%|2         | 688128/26421880 [00:00<00:30, 846817.27it/s] 
      3%|2         | 786432/26421880 [00:00<00:30, 854071.17it/s]
      3%|3         | 884736/26421880 [00:01<00:29, 864830.69it/s]
      4%|3         | 983040/26421880 [00:01<00:29, 872846.70it/s]
      4%|4         | 1081344/26421880 [00:01<00:28, 878798.22it/s]
      4%|4         | 1179648/26421880 [00:01<00:28, 883068.35it/s]
      5%|4         | 1277952/26421880 [00:01<00:28, 886982.34it/s]
      5%|5         | 1376256/26421880 [00:01<00:28, 888949.08it/s]
      6%|5         | 1474560/26421880 [00:01<00:28, 890570.65it/s]
      6%|5         | 1572864/26421880 [00:01<00:27, 893711.66it/s]
      6%|6         | 1671168/26421880 [00:01<00:27, 892965.46it/s]
      7%|6         | 1769472/26421880 [00:02<00:27, 892556.63it/s]
      7%|7         | 1867776/26421880 [00:02<00:27, 892264.26it/s]
      8%|7         | 1998848/26421880 [00:02<00:24, 980258.69it/s]
      8%|7         | 2097152/26421880 [00:02<00:25, 954544.86it/s]
      8%|8         | 2195456/26421880 [00:02<00:25, 936232.77it/s]
      9%|8         | 2326528/26421880 [00:02<00:23, 1009934.43it/s]
      9%|9         | 2457600/26421880 [00:02<00:22, 1061928.75it/s]
     10%|9         | 2588672/26421880 [00:02<00:22, 1051093.96it/s]
     10%|#         | 2719744/26421880 [00:02<00:21, 1090136.28it/s]
     11%|#         | 2850816/26421880 [00:03<00:21, 1117766.98it/s]
     11%|#1        | 2981888/26421880 [00:03<00:25, 917237.96it/s] 
     12%|#1        | 3112960/26421880 [00:03<00:23, 985150.64it/s]
     12%|#2        | 3244032/26421880 [00:03<00:22, 1038074.01it/s]
     13%|#2        | 3375104/26421880 [00:03<00:21, 1078697.95it/s]
     13%|#3        | 3506176/26421880 [00:03<00:21, 1064655.44it/s]
     14%|#3        | 3637248/26421880 [00:03<00:20, 1098318.13it/s]
     14%|#4        | 3768320/26421880 [00:03<00:20, 1122460.25it/s]
     15%|#4        | 3899392/26421880 [00:04<00:19, 1140454.76it/s]
     15%|#5        | 4030464/26421880 [00:04<00:19, 1153604.54it/s]
     16%|#5        | 4161536/26421880 [00:04<00:23, 938536.87it/s] 
     16%|#6        | 4292608/26421880 [00:04<00:22, 1000584.49it/s]
     17%|#6        | 4423680/26421880 [00:04<00:20, 1049687.67it/s]
     17%|#7        | 4554752/26421880 [00:04<00:20, 1088517.91it/s]
     18%|#7        | 4685824/26421880 [00:04<00:19, 1115651.94it/s]
     18%|#8        | 4816896/26421880 [00:04<00:18, 1139304.44it/s]
     19%|#8        | 4947968/26421880 [00:05<00:18, 1154653.03it/s]
     19%|#9        | 5079040/26421880 [00:05<00:18, 1164333.49it/s]
     20%|#9        | 5210112/26421880 [00:05<00:18, 1171562.98it/s]
     20%|##        | 5373952/26421880 [00:05<00:16, 1264154.57it/s]
     21%|##        | 5505024/26421880 [00:05<00:16, 1242140.56it/s]
     21%|##1       | 5668864/26421880 [00:05<00:15, 1314996.44it/s]
     22%|##2       | 5832704/26421880 [00:05<00:15, 1366369.76it/s]
     23%|##2       | 5996544/26421880 [00:05<00:14, 1401152.14it/s]
     23%|##3       | 6160384/26421880 [00:05<00:14, 1427568.42it/s]
     24%|##3       | 6324224/26421880 [00:06<00:13, 1446801.78it/s]
     25%|##4       | 6488064/26421880 [00:06<00:13, 1458928.15it/s]
     25%|##5       | 6651904/26421880 [00:06<00:13, 1467322.29it/s]
     26%|##5       | 6848512/26421880 [00:06<00:12, 1560561.01it/s]
     27%|##6       | 7012352/26421880 [00:06<00:12, 1541019.17it/s]
     27%|##7       | 7208960/26421880 [00:06<00:11, 1614294.65it/s]
     28%|##8       | 7405568/26421880 [00:06<00:11, 1665522.44it/s]
     29%|##8       | 7634944/26421880 [00:06<00:10, 1787849.53it/s]
     30%|##9       | 7831552/26421880 [00:06<00:10, 1788646.33it/s]
     31%|###       | 8060928/26421880 [00:07<00:09, 1874547.17it/s]
     31%|###1      | 8290304/26421880 [00:07<00:09, 1937044.03it/s]
     32%|###2      | 8519680/26421880 [00:07<00:09, 1981051.06it/s]
     33%|###3      | 8749056/26421880 [00:07<00:08, 2006883.75it/s]
     34%|###3      | 8978432/26421880 [00:07<00:08, 2039132.35it/s]
     35%|###4      | 9240576/26421880 [00:07<00:08, 2142308.63it/s]
     36%|###5      | 9502720/26421880 [00:07<00:07, 2214118.07it/s]
     37%|###6      | 9764864/26421880 [00:07<00:07, 2264771.15it/s]
     38%|###8      | 10059776/26421880 [00:07<00:06, 2384189.69it/s]
     39%|###9      | 10354688/26421880 [00:08<00:06, 2469077.14it/s]
     40%|####      | 10649600/26421880 [00:08<00:06, 2536102.46it/s]
     41%|####1     | 10944512/26421880 [00:08<00:05, 2580393.90it/s]
     43%|####2     | 11272192/26421880 [00:08<00:05, 2695704.09it/s]
     44%|####3     | 11599872/26421880 [00:08<00:05, 2780007.29it/s]
     45%|####5     | 11927552/26421880 [00:08<00:05, 2843882.42it/s]
     47%|####6     | 12288000/26421880 [00:08<00:04, 2968448.08it/s]
     48%|####7     | 12648448/26421880 [00:08<00:04, 3060285.12it/s]
     49%|####9     | 13008896/26421880 [00:08<00:04, 3124837.05it/s]
     51%|#####     | 13402112/26421880 [00:09<00:04, 3253980.92it/s]
     52%|#####2    | 13795328/26421880 [00:09<00:03, 3349171.04it/s]
     54%|#####3    | 14188544/26421880 [00:09<00:03, 3422373.57it/s]
     55%|#####5    | 14614528/26421880 [00:09<00:03, 3556853.10it/s]
     57%|#####6    | 15040512/26421880 [00:09<00:03, 3648767.74it/s]
     59%|#####8    | 15499264/26421880 [00:09<00:02, 3798064.68it/s]
     60%|######    | 15958016/26421880 [00:09<00:02, 3908626.20it/s]
     62%|######2   | 16416768/26421880 [00:09<00:02, 3989956.27it/s]
     64%|######3   | 16908288/26421880 [00:09<00:02, 4131673.66it/s]
     66%|######5   | 17399808/26421880 [00:10<00:02, 4230399.68it/s]
     68%|######7   | 17924096/26421880 [00:10<00:01, 4382854.95it/s]
     70%|######9   | 18448384/26421880 [00:10<00:01, 4503133.82it/s]
     72%|#######1  | 19005440/26421880 [00:10<00:01, 4661755.34it/s]
     74%|#######4  | 19562496/26421880 [00:10<00:01, 4785026.19it/s]
     76%|#######6  | 20152320/26421880 [00:10<00:01, 4947185.12it/s]
     79%|#######8  | 20742144/26421880 [00:10<00:01, 5072779.15it/s]
     81%|########  | 21364736/26421880 [00:10<00:00, 5240572.75it/s]
     83%|########3 | 21987328/26421880 [00:10<00:00, 5373932.86it/s]
     86%|########5 | 22642688/26421880 [00:11<00:00, 5538943.67it/s]
     88%|########8 | 23330816/26421880 [00:11<00:00, 5741809.69it/s]
     91%|######### | 24018944/26421880 [00:11<00:00, 5895138.77it/s]
     94%|#########3| 24707072/26421880 [00:11<00:00, 6024386.54it/s]
     96%|#########6| 25460736/26421880 [00:11<00:00, 6247937.81it/s]
     99%|#########9| 26214400/26421880 [00:11<00:00, 6427236.60it/s]
    100%|##########| 26421880/26421880 [00:11<00:00, 2284345.60it/s]
    Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw

    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz
    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz

      0%|          | 0/29515 [00:00<?, ?it/s]
    100%|##########| 29515/29515 [00:00<00:00, 272657.34it/s]
    100%|##########| 29515/29515 [00:00<00:00, 271200.48it/s]
    Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw

    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz
    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz

      0%|          | 0/4422102 [00:00<?, ?it/s]
      1%|          | 32768/4422102 [00:00<00:14, 300011.69it/s]
      1%|1         | 65536/4422102 [00:00<00:14, 298767.96it/s]
      3%|2         | 131072/4422102 [00:00<00:09, 434939.14it/s]
      4%|4         | 196608/4422102 [00:00<00:08, 498810.16it/s]
      9%|8         | 393216/4422102 [00:00<00:04, 964271.12it/s]
     12%|#1        | 524288/4422102 [00:00<00:03, 1042089.34it/s]
     16%|#5        | 688128/4422102 [00:00<00:03, 1188392.03it/s]
     19%|#8        | 819200/4422102 [00:00<00:03, 1190286.95it/s]
     22%|##2       | 983040/4422102 [00:00<00:02, 1284428.74it/s]
     26%|##5       | 1146880/4422102 [00:01<00:02, 1348336.08it/s]
     30%|##9       | 1310720/4422102 [00:01<00:02, 1392034.66it/s]
     33%|###3      | 1474560/4422102 [00:01<00:02, 1422098.98it/s]
     37%|###7      | 1638400/4422102 [00:01<00:01, 1443593.18it/s]
     41%|####1     | 1835008/4422102 [00:01<00:01, 1545328.26it/s]
     45%|####5     | 1998848/4422102 [00:01<00:01, 1529306.77it/s]
     49%|####8     | 2162688/4422102 [00:01<00:01, 1521418.32it/s]
     53%|#####3    | 2359296/4422102 [00:01<00:01, 1598568.33it/s]
     57%|#####7    | 2523136/4422102 [00:01<00:01, 1570934.51it/s]
     62%|######1   | 2719744/4422102 [00:02<00:01, 1634334.30it/s]
     66%|######5   | 2916352/4422102 [00:02<00:00, 1678987.17it/s]
     70%|#######   | 3112960/4422102 [00:02<00:00, 1709775.19it/s]
     75%|#######4  | 3309568/4422102 [00:02<00:00, 1734014.66it/s]
     79%|#######9  | 3506176/4422102 [00:02<00:00, 1751595.57it/s]
     84%|########3 | 3702784/4422102 [00:02<00:00, 1760988.64it/s]
     88%|########8 | 3899392/4422102 [00:02<00:00, 1769882.58it/s]
     93%|#########2| 4096000/4422102 [00:02<00:00, 1776011.30it/s]
     97%|#########7| 4292608/4422102 [00:02<00:00, 1783240.70it/s]
    100%|##########| 4422102/4422102 [00:02<00:00, 1483777.26it/s]
    Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw

    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz
    Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz

      0%|          | 0/5148 [00:00<?, ?it/s]
    100%|##########| 5148/5148 [00:00<00:00, 24044851.88it/s]
    Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw





.. GENERATED FROM PYTHON SOURCE LINES 58-61

We pass the ``Dataset`` as an argument to ``DataLoader``. This wraps an iterable over our dataset, and supports
automatic batching, sampling, shuffling and multiprocess data loading. Here we define a batch size of 64, i.e. each element
in the dataloader iterable will return a batch of 64 features and labels.

.. GENERATED FROM PYTHON SOURCE LINES 61-73

.. code-block:: default


    batch_size = 64

    # Create data loaders.
    train_dataloader = DataLoader(training_data, batch_size=batch_size)
    test_dataloader = DataLoader(test_data, batch_size=batch_size)

    for X, y in test_dataloader:
        print(f"Shape of X [N, C, H, W]: {X.shape}")
        print(f"Shape of y: {y.shape} {y.dtype}")
        break





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])
    Shape of y: torch.Size([64]) torch.int64




.. GENERATED FROM PYTHON SOURCE LINES 74-76

Read more about `loading data in PyTorch <data_tutorial.html>`_.


.. GENERATED FROM PYTHON SOURCE LINES 78-80

--------------


.. GENERATED FROM PYTHON SOURCE LINES 82-88

Creating Models
------------------
To define a neural network in PyTorch, we create a class that inherits
from `nn.Module <https://pytorch.org/docs/stable/generated/torch.nn.Module.html>`_. We define the layers of the network
in the ``__init__`` function and specify how data will pass through the network in the ``forward`` function. To accelerate
operations in the neural network, we move it to the GPU or MPS if available.

.. GENERATED FROM PYTHON SOURCE LINES 88-120

.. code-block:: default


    # Get cpu, gpu or mps device for training.
    device = (
        "cuda"
        if torch.cuda.is_available()
        else "mps"
        if torch.backends.mps.is_available()
        else "cpu"
    )
    print(f"Using {device} device")

    # Define model
    class NeuralNetwork(nn.Module):
        def __init__(self):
            super().__init__()
            self.flatten = nn.Flatten()
            self.linear_relu_stack = nn.Sequential(
                nn.Linear(28*28, 512),
                nn.ReLU(),
                nn.Linear(512, 512),
                nn.ReLU(),
                nn.Linear(512, 10)
            )

        def forward(self, x):
            x = self.flatten(x)
            logits = self.linear_relu_stack(x)
            return logits

    model = NeuralNetwork().to(device)
    print(model)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Using cuda device
    NeuralNetwork(
      (flatten): Flatten(start_dim=1, end_dim=-1)
      (linear_relu_stack): Sequential(
        (0): Linear(in_features=784, out_features=512, bias=True)
        (1): ReLU()
        (2): Linear(in_features=512, out_features=512, bias=True)
        (3): ReLU()
        (4): Linear(in_features=512, out_features=10, bias=True)
      )
    )




.. GENERATED FROM PYTHON SOURCE LINES 121-123

Read more about `building neural networks in PyTorch <buildmodel_tutorial.html>`_.


.. GENERATED FROM PYTHON SOURCE LINES 126-128

--------------


.. GENERATED FROM PYTHON SOURCE LINES 131-135

Optimizing the Model Parameters
----------------------------------------
To train a model, we need a `loss function <https://pytorch.org/docs/stable/nn.html#loss-functions>`_
and an `optimizer <https://pytorch.org/docs/stable/optim.html>`_.

.. GENERATED FROM PYTHON SOURCE LINES 135-140

.. code-block:: default


    loss_fn = nn.CrossEntropyLoss()
    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)









.. GENERATED FROM PYTHON SOURCE LINES 141-143

In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and
backpropagates the prediction error to adjust the model's parameters.

.. GENERATED FROM PYTHON SOURCE LINES 143-163

.. code-block:: default


    def train(dataloader, model, loss_fn, optimizer):
        size = len(dataloader.dataset)
        model.train()
        for batch, (X, y) in enumerate(dataloader):
            X, y = X.to(device), y.to(device)

            # Compute prediction error
            pred = model(X)
            loss = loss_fn(pred, y)

            # Backpropagation
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

            if batch % 100 == 0:
                loss, current = loss.item(), (batch + 1) * len(X)
                print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")








.. GENERATED FROM PYTHON SOURCE LINES 164-165

We also check the model's performance against the test dataset to ensure it is learning.

.. GENERATED FROM PYTHON SOURCE LINES 165-181

.. code-block:: default


    def test(dataloader, model, loss_fn):
        size = len(dataloader.dataset)
        num_batches = len(dataloader)
        model.eval()
        test_loss, correct = 0, 0
        with torch.no_grad():
            for X, y in dataloader:
                X, y = X.to(device), y.to(device)
                pred = model(X)
                test_loss += loss_fn(pred, y).item()
                correct += (pred.argmax(1) == y).type(torch.float).sum().item()
        test_loss /= num_batches
        correct /= size
        print(f"Test Error: \n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \n")








.. GENERATED FROM PYTHON SOURCE LINES 182-185

The training process is conducted over several iterations (*epochs*). During each epoch, the model learns
parameters to make better predictions. We print the model's accuracy and loss at each epoch; we'd like to see the
accuracy increase and the loss decrease with every epoch.

.. GENERATED FROM PYTHON SOURCE LINES 185-193

.. code-block:: default


    epochs = 5
    for t in range(epochs):
        print(f"Epoch {t+1}\n-------------------------------")
        train(train_dataloader, model, loss_fn, optimizer)
        test(test_dataloader, model, loss_fn)
    print("Done!")





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Epoch 1
    -------------------------------
    loss: 2.304781  [   64/60000]
    loss: 2.279226  [ 6464/60000]
    loss: 2.262871  [12864/60000]
    loss: 2.258641  [19264/60000]
    loss: 2.240942  [25664/60000]
    loss: 2.205658  [32064/60000]
    loss: 2.219674  [38464/60000]
    loss: 2.176857  [44864/60000]
    loss: 2.183223  [51264/60000]
    loss: 2.146680  [57664/60000]
    Test Error: 
     Accuracy: 50.9%, Avg loss: 2.135962 

    Epoch 2
    -------------------------------
    loss: 2.156292  [   64/60000]
    loss: 2.127022  [ 6464/60000]
    loss: 2.066055  [12864/60000]
    loss: 2.082187  [19264/60000]
    loss: 2.025924  [25664/60000]
    loss: 1.964080  [32064/60000]
    loss: 2.001160  [38464/60000]
    loss: 1.906679  [44864/60000]
    loss: 1.923958  [51264/60000]
    loss: 1.850588  [57664/60000]
    Test Error: 
     Accuracy: 53.4%, Avg loss: 1.837593 

    Epoch 3
    -------------------------------
    loss: 1.883808  [   64/60000]
    loss: 1.835114  [ 6464/60000]
    loss: 1.713934  [12864/60000]
    loss: 1.759387  [19264/60000]
    loss: 1.657956  [25664/60000]
    loss: 1.607270  [32064/60000]
    loss: 1.646671  [38464/60000]
    loss: 1.540018  [44864/60000]
    loss: 1.575470  [51264/60000]
    loss: 1.473858  [57664/60000]
    Test Error: 
     Accuracy: 61.5%, Avg loss: 1.484963 

    Epoch 4
    -------------------------------
    loss: 1.556970  [   64/60000]
    loss: 1.516497  [ 6464/60000]
    loss: 1.370513  [12864/60000]
    loss: 1.444114  [19264/60000]
    loss: 1.333008  [25664/60000]
    loss: 1.324762  [32064/60000]
    loss: 1.352820  [38464/60000]
    loss: 1.274232  [44864/60000]
    loss: 1.310185  [51264/60000]
    loss: 1.212210  [57664/60000]
    Test Error: 
     Accuracy: 63.6%, Avg loss: 1.236480 

    Epoch 5
    -------------------------------
    loss: 1.311336  [   64/60000]
    loss: 1.292235  [ 6464/60000]
    loss: 1.134207  [12864/60000]
    loss: 1.236909  [19264/60000]
    loss: 1.117082  [25664/60000]
    loss: 1.137494  [32064/60000]
    loss: 1.168772  [38464/60000]
    loss: 1.106078  [44864/60000]
    loss: 1.142242  [51264/60000]
    loss: 1.055142  [57664/60000]
    Test Error: 
     Accuracy: 65.0%, Avg loss: 1.077993 

    Done!




.. GENERATED FROM PYTHON SOURCE LINES 194-196

Read more about `Training your model <optimization_tutorial.html>`_.


.. GENERATED FROM PYTHON SOURCE LINES 198-200

--------------


.. GENERATED FROM PYTHON SOURCE LINES 202-205

Saving Models
-------------
A common way to save a model is to serialize the internal state dictionary (containing the model parameters).

.. GENERATED FROM PYTHON SOURCE LINES 205-211

.. code-block:: default


    torch.save(model.state_dict(), "model.pth")
    print("Saved PyTorch Model State to model.pth")







.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Saved PyTorch Model State to model.pth




.. GENERATED FROM PYTHON SOURCE LINES 212-217

Loading Models
----------------------------

The process for loading a model includes re-creating the model structure and loading
the state dictionary into it.

.. GENERATED FROM PYTHON SOURCE LINES 217-221

.. code-block:: default


    model = NeuralNetwork().to(device)
    model.load_state_dict(torch.load("model.pth"))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    <All keys matched successfully>



.. GENERATED FROM PYTHON SOURCE LINES 222-223

This model can now be used to make predictions.

.. GENERATED FROM PYTHON SOURCE LINES 223-246

.. code-block:: default


    classes = [
        "T-shirt/top",
        "Trouser",
        "Pullover",
        "Dress",
        "Coat",
        "Sandal",
        "Shirt",
        "Sneaker",
        "Bag",
        "Ankle boot",
    ]

    model.eval()
    x, y = test_data[0][0], test_data[0][1]
    with torch.no_grad():
        x = x.to(device)
        pred = model(x)
        predicted, actual = classes[pred[0].argmax(0)], classes[y]
        print(f'Predicted: "{predicted}", Actual: "{actual}"')






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Predicted: "Ankle boot", Actual: "Ankle boot"




.. GENERATED FROM PYTHON SOURCE LINES 247-249

Read more about `Saving & Loading your model <saveloadrun_tutorial.html>`_.



.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 1 minutes  5.250 seconds)


.. _sphx_glr_download_beginner_basics_quickstart_tutorial.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: quickstart_tutorial.py <quickstart_tutorial.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: quickstart_tutorial.ipynb <quickstart_tutorial.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
